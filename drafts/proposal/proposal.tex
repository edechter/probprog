\documentclass{article}

\RequirePackage[top=1.2in,bottom=1.2in,left=1.5in,right=1.5in]{geometry}
\RequirePackage{subcaption}
\RequirePackage[labelfont=bf]{caption}
\RequirePackage{float}

\RequirePackage{amsmath,amsfonts,amssymb,amsthm}
\RequirePackage{algorithm}
\RequirePackage[noend]{algpseudocode}
\RequirePackage{graphicx}
\RequirePackage[margin=0.5in]{caption}
\RequirePackage[colorlinks=true]{hyperref}
\RequirePackage{appendix}
\RequirePackage{url}
\RequirePackage{cleveref}

\begin{document}
    \begin{center}
        \Large
        6.945 Project Proposal\\
        \vspace{0.2in}
        \normalsize
        Eyal Dechter, Matthew Johnson, Zenna Tavares\\
        \vspace{0.1in}
        \footnotesize
        Revised \today
    \end{center}

    \section{Domain of interest: probabilistic programming}
    In recent years, multiple probabilistic programming languages have emerged
    with the goal of merging declarative programming with probabilistic
    modeling. These languages seek to build abstractions in part so that the
    task of probabilistic modeling can be separated from the zoo of inference
    algorithms available. Often these languages either focus on a particular
    inference algorithm or restricted set of probability distributions (such as
    STAN \cite{stan} or BUGS \cite{bugs}) or employ exceedingly general inference algorithms so
    that any probabilistic model can be expressed at the expense of efficiency
    or even tractability (like Church \cite{church} or BLOG \cite{blog}).

    We aim to understand the design and implementation of probabilistic
    programming languages. In particular, we hope to identify abstractions that
    can allow a fully general probabilistic programming language to leverage
    special model structures for inference so that its capabilities can be
    easily extended. We'd also like to understand why no existing probabilistic
    programming languages seem to attempt such abstractions.

    \section{A plausible decomposition}
    \begin{enumerate}
        \item \textbf{Modeling language}: means for expressing probability
            distributions and mapping those expressions into representations
            that enable analysis. One approach is to accept expressions that
            describe the generative process that gives rise to the data (as in
            Church), but there are other useful distributions to include that
            are not easily expressed as generative models (such as constraints
            or Markov random fields).
        \item \textbf{Analysis}: mechanisms for recognizing known tractable
            structure when present and choosing good inference algorithms to
            apply. This process is necessarily heuristic and should be made
            extensible so that new structures or algorithms can be added.
        \item \textbf{Inference engines}: implementation of inference
            algorithms that can operate on the modeling language abstraction.
            The set of algorithms must also be extensible.
    \end{enumerate}

    \section{Plan for implementation}
    Probabilistic programming is not well understood and is an area of active
    research, so it is probable that our understanding of the tasks involved in
    our project and even our project goals themselves may change as we build
    the system. Therefore, these are our current best guesses at a plan for
    implementing something reasonable.

    We will jointly design and implement the base system, consisting of a basic
    representation of models and a generic inference algorithm (such as the
    Metropolis-Hastings approach in Church). With that common base, here is an
    assignment of parts:
    \begin{itemize}
    \item \textbf{Zenna} will focus at first on improving model representations
        and methods for automatically identifying tractable classes, either
        analytically or by inspecting program traces.
    \item \textbf{Eyal} will focus at first on representations of and
        algorithms for discrete models. Some tractable classes include discrete
        graphical models with small treewidth, and algorithms include many
        sampling methods, (loopy) belief propagation, and the junction tree
        algorithm.
    \item \textbf{Matt} will focus at first on model manipulation and inference
        for linear Gaussian models. Linear Gaussian inference is extremely
        powerful, mapping to linear algebraic operations, and can be extended
        to some nonlinear systems (common in robotics) by applying techniques
        like automatic differentiation.
    \end{itemize}

    \bibliographystyle{plain}
    \bibliography{refs}
\end{document}

